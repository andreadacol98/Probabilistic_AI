% -*- root: Main.tex -*-
\section{Gaussian Processes}
$f\sim GP(\mu,k)\Rightarrow\forall \left\{x_1, \dots, x_n\right\} \,\forall n < \infty$\\
$\left[f(x_1)\dots f(x_n)\right]\sim N(\left[\mu(x_1)\dots \mu(x_n)\right], K)$\\ where $K_{ij}=k(x_i,x_j)$
% A GP $\{X_t\}_t$ is a collection of random variables from which any finite sample has a joint Gaussian distribution.\\
% For any finite set of points $T=\{t_1, \dots, t_n\}$ from a GP, it hold that $(X_{t_1}, \dots,X_{t_n})\sim \mathcal{N}(\pmb{\mu_T},\pmb{\Sigma_T})$ with $\pmb{\mu_T} = (\mu(t_1),\dots,\mu(t_n))$, $\pmb{\Sigma_T}(i,j)=k(X_{t_i},X_{t_j})$
\subsection{Gaussian Process Regression}
$f\sim GP(\mu,k)$ then: $f\vert y_{1:n},x_{1:n} \sim GP(\tilde{\mu},\tilde{k})$\\
$\tilde{\mu}(x) = \mu(x) + K_{A, x}^T{(K_{AA}+\epsilon I_n)}^{-1}\left(y_A-\mu_A\right)$\\
$\tilde{k}(x,x') = k(x,x') - K_{A,x}^T{(K_{A}+\epsilon I_n)}^{-1} K_{A,x'}$\\
Where: $K_{A,z} = {\left[k(x_1,z)\dots k(x_n,z)\right]}^T$\\
${\left[K_{AA}\right]}_{ij} = k(x_i, x_j)$ and $\mu_A = {\left[\mu(x_1 \dots x_n)\right]}^T$


%$p\big(\begin{bmatrix}
%\mathbf{y} \\
%y^*\\
%\end{bmatrix}|x^*,\mathbf{X}, \sigma \big) = \mathcal{N}\big(\begin{bmatrix}
%\mathbf{y} \\
%y^*\\
%\end{bmatrix} | \mathbf{0},\begin{bmatrix}
%\mathbf{C_n} & \mathbf{k} \\
%\mathbf{k}^T & c 
%\end{bmatrix}  \big)$\\
%with $\mathbf{C_n} = \mathbf{K} + \sigma^2 \mathbf{I}, c = k(x_{n+1},x_{n+1}) + \sigma^2,\\
%\mathbf{k}=k(x_{n+1}, \mathbf{X}), \mathbf{K}=k(\mathbf{X}, \mathbf{X})$\\
%$p(y^*|x^*, X, y) = \mathcal{N}(y^*|\mu, \sigma^2)$\\
%with $\mu = k^T C_n^{-1}y, \sigma^2 = c-k^TC_n^{-1}k$\\

%\subsection*{GP Hyperparameter Optimization}
%Log-likelihood:\\
%$l(Y|\theta) = -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log |C_n| - \frac{1}{2} Y^T C_n^{-1}Y$\\
%Set of hyperparameters $\theta$ determine parameters $C_n$. Gradient descent: $\nabla_{\theta_i}l(Y|\theta) = -\frac{1}{2}tr(C_n^{-1} \frac{\partial C_n}{\partial \theta_i}) + \frac{1}{2} Y^T C_n^{-1} \frac{\partial C_n}{\partial \theta_i} C_n^{-1} Y$

\subsection{Kernels}
	$k(x,y)$ is a kernel if it's symmetric semidefinite positive:\\
	$\forall \left\{x_1, \dots, x_n \right\}$ then for the Gram Matrix \\
	${\left[K\right]}_{ij}=k(x_i,x_j)$ holds $c^TKc\geq0\forall c$\\
    \textbf{Some Kernels:} (h is the bandwidth hyperp.)\\
    Gaussian (rbf): $k(x,y) = \exp( -\tfrac{||x-y||^2}{h^2})$\\
    Exponential: $k(x,y) = \exp( -\tfrac{||x-y||}{h})$\\
    Linear kernel: $k(x,y) = x^Ty$\\
\subsection{Optimization of Kernel Parameters}
Given a dataset $A$, a kernel function $k(x,y;\theta)$. $y\sim N(0, K_y(\theta))$ where $K_y(\theta)=K_{AA}(\theta)+\sigma_n^2I$\\
$\hat\theta = \argmax_\theta \log p(y\vert X;\theta)$\\ 
In GP: $\hat\theta=\argmin_\theta y^TK_{y}^{-1}(\theta)y + \log\vert K_y(\theta)\vert$\\
We can from here $\nabla\downarrow$:\\
$\nabla_\theta \log p(y\vert X;\theta) = ${\scriptsize$\frac{1}{2}tr\left(\left(\alpha\alpha^T-K^{-1}\right)\frac{\partial K}{\partial \theta}\right)$, $\alpha = K^{-1}y$}\\
Or we could also be baysian about $\theta$
\subsection{Aproximation Techniques}


% \subsubsection*{Polynomial kernel}
% $k_1 = (x^Ty)^m$ represents monomial of deg m \\
% $k_2 = (1+x^Ty)^m$ represents monomials up to deg m

%\subsubsection*{Properties of kernel}
%\begin{inparaitem}
%	\item k must be symmetric\\
%	\item the kernel matrix must be SPD
%\end{inparaitem}

%\subsubsection*{Kernel matrix}
%The kernel matrix $K$ is SPD \\
%$K = 
%\begin{bmatrix}
%k(x_1,x_1) & \dots & k(x_1,x_n) \\
%\vdots & \ddots & \vdots \\
%k(x_n, x_1) & \dots & k(x_n,x_n)
%\end{bmatrix}$\\
%$\left ( XX^T \right )$ for inner product as kernel.

% \subsubsection*{semi-positive-definite matrices}
% $M \in \mathbb{R}^{n\times n}$ is SPD $\Leftrightarrow$\\
% $\forall x \in \mathbb{R}^n: x^TMx \geq 0 \Leftrightarrow$\\
% all eigenvalues of $M$ are positive $\geq 0$

%\subsection*{Nearest Neighbor k-NN}
%$y=sign(\sum \limits_{i=1}^n y_i [x_i \text{ among k nearest neighbors of } x])$

%$k_1(x,y) + k_2(x,y)$ , 
%$k_1(x,y) \cdot k_2(x,y)$, 
%$c \cdot k_1(x,y)$ for $c>0$ , 
%$f(k_1(x,y))$, where $f$ is exponential/polynomial with positive coefficents\\
%$k(x,y) = \phi(x)^T \phi(y)$, for some $\phi$ ass. with k

%\subsubsection*{Parametric vs. Nonparametric}
%\emph{Parametric}: have finite set of parameters\\
%E.g. linear regression, perceptron,...\\
%$f(x) = w^Tx, w\in \mathbb{R}^d$ (d is independent of #data)
% \begin{itemize}
% 	\item[+] computationally not complex
% \end{itemize}

%\emph{Nonparametric}: grows in complexity with the size of the data\\
%E.g. kernelized Perceptron, k-NN,...\\
%$f(x) = \sum_{i=1}^n \alpha_i y_i k(x_i,x_n)$ (depends on #data)\\
% \begin{itemize}
% 	\item[+] potentially much more expressive.
% \end{itemize}